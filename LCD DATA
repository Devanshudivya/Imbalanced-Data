library(mlr)
library(randomForest)
library(corrplot)
library(e1071)
#paid-"Current","Fully Paid","In Grace Period","Late (16-30 days)","Late (31-120 days)","Current" 
#Non-Paid-"Charged Off","Default" 
claim<-read.csv(file.choose(),header = TRUE,na.strings=c(" ","NA"))
claim$bad_loans<-ifelse(claim$bad_loans==1,"yes","no")
claim$bad_loans<-as.factor(claim$bad_loans)
dim(claim) #466287*68
loandata<-claim
claim<-loandata
dim(loandata)

dim(claim)
ggplot(loandata, aes(loan_amnt, col = purpose)) + geom_histogram(bins = 50) + facet_grid(grade ~ .)
table(claim$loan_status,claim$bad_loans)
summarizeColumns(claim)
#str(claim)
claim<-claim[,-c(1,2,11,16,17,19,20,22,23,24,27,46,48,49,52,64)]
summarizeColumns(claim)

############### Removing missing values
claim$annual_inc[is.na(claim$annual_inc)] <- mean(claim$annual_inc, na.rm = TRUE)
claim<-subset(claim, !is.na(claim$delinq_2yrs))
claim<-subset(claim, !is.na(claim$collections_12_mths_zero))
claim<-claim[,-c(18,19,37)]
#######################Check for the corelation using cor plot--make new matrix 
str(claim)
summarizeColumns(claim)
prop.table(table(claim$bad_loans))
summarizeColumns(claim)
library(caret)
corplt <-findCorrelation(x = cor(claim[,c(1:3,5,6,11,15:22,24:35,37:48)]),cutoff = 0.7)
corplt
claim<-claim[,-corplt]
summarizeColumns(claim)
claim$status<-as.character(claim$status)
claim$status[claim$status %in% c("Current","Fully Paid", "In Grace Period","Late (16-30 days)","Late (31-120 days)" )]<-"Paid"
claim$status[claim$status %in% c("Charged Off", "Default")]<-"Not Paid"
head(claim$status,10)
claim$status<-as.factor(claim$status)


dim(claim)
claim1<-claim[,-c(25,37)]
dim(claim)
names(claim)
#train<-new.claim[1:326000,]
#test<-new.claim[326001:466142,]
summarizeColumns(claim)[,"nlevs"]
#sum(train$bad_loans=="yes")
##Building Model
#sum(claim$bad_loans=="yes")
table()
for(i in names(claim1)){
  p <- 5/100
  ld <- names(which(prop.table(table(claim1[[i]])) < p))
  levels(claim1[[i]])[levels(claim1[[i]]) %in% ld] <- "Other"
}
dim(claim)
dim(claim1)

status<-claim$status
bad_loans<-claim$bad_loans
head(claim2)
claim2<-cbind(claim1,status,bad_loans)
str(claim2)
train.claim2<-claim2[1:326000,]
library(dplyr)
set.seed(121)
claim3<-sample_n(train.claim2, 200000)

test.claim2<-claim2[326001:466142,]
dim(claim3)
train.task <- makeClassifTask(data = claim3,target = "bad_loans")
test.task<-makeClassifTask(data = test.claim2,target = "bad_loans")
test.task
library(mlr)
library(FSelector)
#train.task<-removeConstantFeatures(train.task)
#test.task<-removeConstantFeatures(test.task)
train.task
test.task
var_imp <- generateFilterValuesData(train.task, method = c("information.gain"))
plotFilterValues(var_imp,feat.type.cols = TRUE)
xtrain.task
dim(claim)
names(claim)
summarizeColumns(claim)
summarizeColumns(claim2)[,"nlevs"]
rm(claim,claim1)
?smote
dim(claim2)
 
system.time(
  train.smote <- smote(train.task,rate = 10,nn = 5)
)

table(getTaskTargets(train.smote))
prop.table(table(getTaskTargets(train.smote)))
listLearners("classif","twoclass")[c("class","package")]
###Logistics Regression
lrn <- makeLearner('classif.logreg', predict.type = 'prob')
lrn$par.vals<- list(laplace = 1)
#10fold CV - stratified
folds <- makeResampleDesc("CV",iters=10,stratify = TRUE)
#cross validation function
fun_cv <- function(a){
  crv_val <- resample(lrn,a,folds,measures = list(acc,tpr,tnr,fpr,fp,fn))
  crv_val$aggr
}
fun_cv(train.smote)
##########################Naive bayes
naive_learner <- makeLearner("classif.naiveBayes",predict.type = "response")
naive_learner$par.vals <- list(laplace = 1)
#10fold CV - stratified
folds <- makeResampleDesc("CV",iters=10,stratify = TRUE)
#cross validation function
fun_cv <- function(a){
  crv_val <- resample(naive_learner,a,folds,measures = list(acc,tpr,tnr,fpr,fp,fn))
  crv_val$aggr
}
fun_cv(train.task)
#acc.test.mean tpr.test.mean tnr.test.mean fpr.test.mean  fp.test.mean  fn.test.mean 
#0.98785000    0.99230190    0.92349943    0.07650057   99.00000000  144.00000000 
fun_cv(train.smote)
#acc.test.mean tpr.test.mean tnr.test.mean fpr.test.mean  fp.test.mean  fn.test.mean 
#0.98569844    0.99340850    0.97455374    0.02544626  329.30000000  123.30000000 
prop.table(table(getTaskTargets(train.smote)))
library(randomForest)
#train and predict
nB_model <- train(naive_learner, train.smote)
nB_predict <- predict(nB_model,test.task) 

#xgboost
set.seed(2002)
xgb_learner <- makeLearner("classif.xgboost",predict.type = "response")
xgb_learner$par.vals <- list(objective = "binary:logistic",eval_metric = "error",nrounds = 150,print.every.n = 50)
#define hyperparameters for tuning
xg_ps <- makeParamSet( 
  makeIntegerParam("max_depth",lower=3,upper=10),
  makeNumericParam("lambda",lower=0.05,upper=0.5),
  makeNumericParam("eta", lower = 0.01, upper = 0.5),
  makeNumericParam("subsample", lower = 0.50, upper = 1),
  makeNumericParam("min_child_weight",lower=2,upper=10),
  makeNumericParam("colsample_bytree",lower = 0.50,upper = 0.80)
)
#define search function
rancontrol <- makeTuneControlRandom(maxit = 5L) #do 5 iterations
#5 fold cross validation
set_cv <- makeResampleDesc("CV",iters = 5L,stratify = TRUE)
#tune parameters
xgb_tune <- tuneParams(learner = xgb_learner, task = train.task, resampling = set_cv, measures = list(acc,tpr,tnr,fpr,fp,fn), par.set = xg_ps, control = rancontrol)
#set optimal parameters
xgb_new <- setHyperPars(learner = xgb_learner, par.vals = xgb_tune$x)
xgmodel <- train(xgb_new, train.task)
